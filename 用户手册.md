# AI编程产品评测工作流系统 - 用户手册

**版本**: 1.0.0  
**作者**: Manus AI  
**更新日期**: 2025年8月6日

## 目录

1. [系统概述](#系统概述)
2. [快速开始](#快速开始)
3. [功能详解](#功能详解)
4. [配置指南](#配置指南)
5. [使用教程](#使用教程)
6. [故障排除](#故障排除)
7. [API文档](#api文档)
8. [最佳实践](#最佳实践)

## 系统概述

AI编程产品评测工作流系统是一个专为评测各类AI编程助手产品而设计的自动化平台。该系统能够根据不同的用户画像和编程语言需求，自动生成相应的编程问题，向多个AI产品提问，收集回答，进行智能评分，并将结果同步到飞书多维表格中。

### 核心特性

**智能问题生成**: 系统基于用户画像（计算机专业学生、非计算机专业入门者、前后端算法研发、其他角色）和编程语言（JavaScript、Python、Java、C++、Go、Rust）动态生成针对性的编程问题。问题涵盖学习编程和项目开发两大类型，确保评测的全面性和实用性。

**多平台自动化**: 支持对豆包、Deepseek、Kimi k2、元宝AI编程、千问等主流AI编程产品进行自动化测试。系统通过浏览器自动化技术，模拟真实用户操作，包括页面导航、模型选择、问题输入、回答获取等完整流程。

**智能追问机制**: 系统支持0-3次的智能追问功能，能够根据AI的初始回答质量，自动生成深入的追问，以更全面地评估AI产品的能力边界和回答质量。

**五档评分体系**: 采用0-4分的五档评分标准，从"完全不可用"到"满分"，每个评分都有明确的用户体验角度描述，确保评分的客观性和一致性。

**飞书集成**: 无缝集成飞书多维表格，所有评测结果、评分理由、追问记录等数据都会自动同步，便于团队协作和数据分析。

### 技术架构

系统采用现代化的前后端分离架构，后端基于Node.js + TypeScript + Express构建，前端使用React + TypeScript开发，数据存储采用SQLite + 飞书多维表格的混合方案，浏览器自动化基于Puppeteer实现。

## 快速开始

### 系统要求

- **操作系统**: Ubuntu 20.04+ / macOS 10.15+ / Windows 10+
- **Node.js**: 18.0+
- **内存**: 最低4GB，推荐8GB+
- **存储**: 最低2GB可用空间
- **网络**: 稳定的互联网连接

### 安装步骤

1. **克隆项目**
```bash
git clone <repository-url>
cd ai-coding-evaluator
```

2. **安装后端依赖**
```bash
cd backend
npm install
```

3. **安装前端依赖**
```bash
cd ../frontend
npm install
```

4. **配置环境变量**
```bash
cd ../backend
cp .env.example .env
# 编辑.env文件，填入必要的配置信息
```

5. **启动服务**
```bash
# 启动后端服务
cd backend
npm run dev

# 启动前端服务（新终端）
cd frontend
npm run dev
```

6. **访问系统**
打开浏览器访问 `http://localhost:5173`

### 首次配置

首次使用系统时，需要完成以下配置：

**飞书集成配置**: 在设置页面中配置飞书应用的App ID、App Secret和多维表格ID。这些信息可以从飞书开放平台获取。

**AI产品验证**: 系统会自动检测各AI产品的可访问性，对于需要登录的产品，系统会提示用户手动登录一次以保存认证状态。

**评分标准确认**: 查看并确认系统预设的评分标准是否符合您的评测需求，可根据实际情况进行调整。

## 功能详解

### 仪表板

仪表板是系统的核心控制中心，提供了系统运行状态的全景视图。主要包含以下信息模块：

**任务统计**: 显示总任务数、运行中任务数、平均评分和总问题数等关键指标。这些数据实时更新，帮助用户快速了解系统的使用情况和评测效果。

**最近任务**: 展示最近创建或执行的任务列表，包括任务名称、状态、进度和操作按钮。用户可以直接从这里快速访问任务详情或执行操作。

**AI产品表现**: 通过柱状图展示各AI产品的平均评分对比，帮助用户直观了解不同产品的整体表现水平。

**评测趋势**: 显示最近7天的评测数据趋势，包括任务数量变化和平均评分变化，帮助用户了解评测活动的规律和AI产品表现的变化趋势。

### 任务管理

**任务列表**: 提供所有评测任务的统一管理界面，支持按状态筛选（全部、运行中、已完成、失败）和关键词搜索。每个任务卡片显示关键信息，包括任务名称、用户画像、编程语言、AI产品数量、问题数量、平均评分等。

**任务创建**: 通过直观的表单界面创建新的评测任务。用户需要填写任务基本信息（名称、描述），选择用户画像和编程语言，选择要评测的AI产品，配置问题类型和追问次数等参数。

**任务执行**: 系统支持手动执行和定时执行两种模式。手动执行适合临时性的评测需求，定时执行适合定期的产品监控和对比分析。

**进度监控**: 实时显示任务执行进度，包括当前执行的问题、已完成的问题数量、预计剩余时间等。用户可以随时暂停、恢复或取消正在执行的任务。

### 结果分析

**详细结果**: 每个任务完成后，系统会生成详细的评测报告，包括每个问题的AI回答、评分、评分理由、追问记录等完整信息。

**对比分析**: 支持同一问题在不同AI产品间的横向对比，以及同一AI产品在不同问题类型上的纵向分析。

**数据导出**: 支持将评测结果导出为Excel、CSV、PDF等多种格式，便于进一步分析和报告制作。

## 配置指南

### 飞书集成配置

飞书集成是系统的重要功能，需要正确配置才能实现数据同步。配置步骤如下：

1. **创建飞书应用**
   - 登录飞书开放平台（https://open.feishu.cn/）
   - 创建企业自建应用
   - 获取App ID和App Secret

2. **配置权限**
   - 在应用管理中添加以下权限：
     - `bitable:app` - 多维表格应用权限
     - `bitable:app:readonly` - 多维表格只读权限
     - `bitable:app:write` - 多维表格写入权限

3. **创建多维表格**
   - 在飞书中创建新的多维表格
   - 设置表格字段，建议包括：任务ID、问题、AI产品、回答、评分、评分理由、时间戳等
   - 获取表格ID（从URL中提取）

4. **系统配置**
   - 在系统设置页面填入App ID、App Secret和表格ID
   - 点击"测试连接"验证配置是否正确

### AI产品配置

系统预配置了主流AI产品的访问信息，但可能需要根据实际情况进行调整：

**豆包配置**: 默认访问https://www.doubao.com/chat/，需要点击"AI编程"按钮进入编程模式。如果页面结构发生变化，可能需要更新选择器配置。

**Kimi配置**: 访问https://www.kimi.com/，支持模型选择（如K2）。系统会自动检测可用模型并进行配置。

**千问配置**: 访问https://chat.qwen.ai/，支持多种功能模式选择，包括Web Dev、Deep Research等。

**元宝AI配置**: 访问https://yuanbao.tencent.com/，支持DeepSeek和DeepThink模型切换。

**Deepseek配置**: 由于Cloudflare保护，可能需要特殊处理。系统会尝试自动绕过验证，但成功率可能受网络环境影响。

### 评分标准配置

系统采用五档评分标准，每档都有明确的定义：

- **0分 - 完全不可用**: 完全不能解决问题，被边缘，以后不会再用
- **1分 - 不可用，存在大量错误内容**: 不能解决问题，被边缘，以后不会再用  
- **2分 - 不可用，存在少量错误内容**: 不能解决问题，但回答中有部分可参考信息，不会主动想起来用
- **3分 - 可用，存在可提升空间**: 能解决问题，有一些小瑕疵，愿意继续使用
- **4分 - 满分**: 能够完美高效的解决问题，愿意推荐给他人使用

用户可以根据实际需求调整评分标准的具体描述，但建议保持五档结构以确保数据的一致性。

## 使用教程

### 创建第一个评测任务

本节将通过一个完整的示例，演示如何创建和执行一个JavaScript学习评测任务。

**步骤1: 任务基本信息**
- 任务名称：JavaScript基础语法评测
- 任务描述：针对计算机专业学生的JavaScript基础语法掌握情况进行评测
- 用户画像：选择"计算机专业学生"
- 编程语言：选择"JavaScript"

**步骤2: AI产品选择**
- 勾选要评测的AI产品：豆包、Kimi、千问、元宝AI
- 系统会显示每个产品的当前状态（正常/异常）

**步骤3: 问题配置**
- 问题类型：选择"学习编程"和"项目开发"
- 追问次数：设置为2次（推荐设置）
- 问题数量：系统会根据配置自动生成适量问题

**步骤4: 执行设置**
- 执行方式：选择"立即执行"
- 超时设置：保持默认30秒
- 截图功能：建议开启，便于问题排查

**步骤5: 提交任务**
点击"创建任务"按钮，系统会自动开始执行评测流程。

### 监控任务执行

任务开始执行后，您可以通过以下方式监控进度：

**实时进度**: 任务详情页面会显示实时进度条，包括当前执行的问题序号、已完成问题数、预计剩余时间等信息。

**执行日志**: 系统会记录详细的执行日志，包括每个步骤的开始时间、结束时间、执行结果等。如果出现错误，日志中会包含详细的错误信息。

**截图记录**: 如果开启了截图功能，系统会在关键步骤自动截图，包括页面导航、问题输入、回答获取等环节。

### 分析评测结果

任务完成后，您可以从多个维度分析评测结果：

**单题分析**: 查看每个问题的详细信息，包括问题内容、各AI产品的回答、评分和评分理由。特别关注评分差异较大的问题，这些往往能反映AI产品的能力差异。

**产品对比**: 通过横向对比功能，查看同一问题在不同AI产品上的表现差异。系统会自动计算各产品的平均分、最高分、最低分等统计指标。

**能力分析**: 根据问题类型（学习编程 vs 项目开发）分析各AI产品的能力特点。某些产品可能在基础概念解释方面表现更好，而另一些产品可能在实际项目问题解决方面更有优势。

**追问效果**: 分析追问的效果，观察AI产品在面对深入问题时的表现变化。优秀的AI产品应该能够在追问中提供更详细、更准确的信息。

## 故障排除

### 常见问题及解决方案

**问题1: 浏览器自动化失败**
- 症状：任务执行时出现"浏览器启动失败"或"页面导航超时"错误
- 原因：可能是系统资源不足、网络连接问题或浏览器配置问题
- 解决方案：
  1. 检查系统内存使用情况，确保有足够的可用内存
  2. 检查网络连接，确保能够正常访问目标AI产品网站
  3. 重启系统服务，清理可能的资源占用
  4. 检查防火墙设置，确保没有阻止浏览器进程

**问题2: AI产品访问异常**
- 症状：某个AI产品始终返回"访问失败"或"响应超时"
- 原因：目标网站可能有反爬虫机制、需要登录验证或服务暂时不可用
- 解决方案：
  1. 手动访问该AI产品网站，确认是否需要登录
  2. 如需登录，在浏览器中完成登录后再执行任务
  3. 检查网站是否有Cloudflare等保护机制
  4. 适当增加超时时间设置
  5. 联系AI产品提供方确认服务状态

**问题3: 飞书同步失败**
- 症状：评测完成但数据没有同步到飞书表格
- 原因：飞书配置错误、权限不足或网络连接问题
- 解决方案：
  1. 在设置页面重新测试飞书连接
  2. 检查App ID、App Secret和表格ID是否正确
  3. 确认飞书应用权限配置是否完整
  4. 检查表格字段结构是否与系统要求匹配
  5. 查看系统日志中的详细错误信息

**问题4: 评分结果异常**
- 症状：所有问题都得到相同评分或评分明显不合理
- 原因：评分引擎配置问题或AI回答解析失败
- 解决方案：
  1. 检查评分标准配置是否正确
  2. 查看具体的AI回答内容，确认是否正确获取
  3. 检查评分引擎的日志输出
  4. 手动重新评分部分问题进行验证
  5. 更新评分引擎的配置参数

### 性能优化建议

**并发控制**: 系统默认最大并发任务数为3个，如果服务器性能较好，可以适当增加这个数值。但要注意，过高的并发可能导致目标网站的反爬虫机制触发。

**超时设置**: 根据网络环境和AI产品的响应速度，适当调整超时时间。网络较慢的环境建议增加到45-60秒。

**截图功能**: 截图功能会占用额外的存储空间和处理时间，如果不需要详细的执行记录，可以关闭此功能以提高性能。

**数据清理**: 定期清理过期的任务数据、日志文件和截图文件，避免占用过多存储空间。

## API文档

系统提供了完整的RESTful API，支持第三方系统集成和自动化操作。

### 认证方式

API使用Bearer Token认证方式，需要在请求头中包含：
```
Authorization: Bearer <your-token>
```

### 核心接口

**任务管理接口**

```http
GET /api/tasks
```
获取任务列表，支持分页和筛选参数。

```http
POST /api/tasks
```
创建新的评测任务，请求体需要包含任务配置信息。

```http
GET /api/tasks/{id}
```
获取指定任务的详细信息，包括执行状态和结果数据。

```http
POST /api/tasks/{id}/execute
```
执行指定任务，支持同步和异步两种模式。

```http
DELETE /api/tasks/{id}
```
删除指定任务及其相关数据。

**配置管理接口**

```http
GET /api/config/profiles
```
获取所有用户画像配置。

```http
GET /api/config/languages
```
获取所有编程语言配置。

```http
GET /api/config/ai-products
```
获取所有AI产品配置和状态信息。

```http
POST /api/config/feishu/test
```
测试飞书集成配置的连通性。

### 响应格式

所有API响应都采用统一的JSON格式：

```json
{
  "success": true,
  "data": {},
  "message": "操作成功",
  "timestamp": "2025-08-06T18:00:00.000Z"
}
```

错误响应格式：

```json
{
  "success": false,
  "error": {
    "code": "TASK_NOT_FOUND",
    "message": "指定的任务不存在"
  },
  "timestamp": "2025-08-06T18:00:00.000Z"
}
```

## 最佳实践

### 评测策略建议

**渐进式评测**: 建议从小规模的评测开始，逐步扩大评测范围。首次使用时，可以选择1-2个AI产品和少量问题进行试验，熟悉系统操作后再进行大规模评测。

**定期监控**: 建立定期评测机制，比如每周或每月对主要AI产品进行一次全面评测，跟踪其能力变化趋势。

**多维度对比**: 不要仅仅关注总体评分，要从不同维度分析AI产品的表现，包括问题类型、编程语言、用户画像等维度。

**结果验证**: 对于评分异常的结果，建议进行人工验证，确保评分的准确性和公正性。

### 数据管理建议

**备份策略**: 定期备份评测数据，包括任务配置、执行结果、截图文件等。建议使用自动化备份工具，确保数据安全。

**版本控制**: 对于重要的评测任务配置，建议进行版本控制，便于追踪配置变更和结果对比。

**数据分析**: 充分利用飞书表格的数据分析功能，创建图表和报表，深入挖掘评测数据的价值。

### 团队协作建议

**权限管理**: 根据团队成员的职责，合理分配系统权限。比如，研发人员可以有完整的配置权限，而业务人员可能只需要查看和分析权限。

**标准化流程**: 建立标准化的评测流程和规范，确保不同人员执行的评测具有可比性。

**知识共享**: 定期分享评测结果和发现，促进团队对AI产品能力的共同理解。

---

*本手册将持续更新，如有问题或建议，请联系技术支持团队。*

